{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "beea6540-f924-4b51-bf14-002d94d59935",
   "metadata": {},
   "source": [
    "### Notebook 4: Gaussian Process for Surrogate Modeling\n",
    "\n",
    "In this notebook, we will demonstrate how to use active learning to make GP model accurately approximate a given limit state. This type of analysis is very useful for probabilistic reliability analysis, where the surrogate model (i.e., a GP model) is tasked with making accurate decisions on whether the values of the system response are greater/lower than the given threshold (i.e., the limit state).\n",
    "\n",
    "This active learning scheme enrich the training dataset sequentially by using a so-called **U** formula:\n",
    "\n",
    "\\begin{equation}\n",
    "U = \\frac{|\\mu - G|}{\\sigma},\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mu$ and $\\sigma$ stand for the GP prediction mean and standard deviation, respective, and $G$ denotes the value of the limit state. By identifying the candidate sample with the minimum **U** value, we are able to greatly improve the GP predictions in the vicinity of the limit state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472bfc04-624f-43ce-8c86-d322e9ed28c9",
   "metadata": {},
   "source": [
    "> GPflow depends on both TensorFlow and TensorFlow Probability, which require very specific versions to be compatible. For example:\n",
    ">\n",
    "> **`!pip install gpflow tensorflow~=2.10.0 tensorflow-probability~=0.18.0`**\n",
    ">\n",
    "> For more details, please refer to the [official documents](https://gpflow.github.io/GPflow/2.9.1/installation.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07c43d3-7215-4b56-8792-ee46954e8479",
   "metadata": {},
   "source": [
    "### 0. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f31cc5e-387a-4fb0-a5b9-e9ba8f75d43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from scipy.stats import qmc\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import gpflow\n",
    "import gpflow.utilities.model_utils as util\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9070827b-0407-4e09-97c4-37585fb32934",
   "metadata": {},
   "source": [
    "### 1. Problem Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb0e54a-4787-4a7d-98a9-21f9efc24e95",
   "metadata": {},
   "source": [
    "We select the following test function in this case study:\n",
    "\n",
    "\\begin{equation}\n",
    "y(x_1, x_2) = 20-(x_1-x_2)^2-8(x_1+x_2-4)^3, \\; x_1 \\in [-5, 5], x_2 \\in [-5, 5]\n",
    "\\end{equation}\n",
    "\n",
    "The failure boundary is defined as $y(x_1, x_2) = 0$.\n",
    "\n",
    "Later on, we will train a Gaussian Process model to capture the failure boundary. This is crucial for accurate and robust risk analysis. To minimize the number of employed training samples, we will use an active learning strategy to allocate new samples in the vicinity of the failure boundary. \n",
    "\n",
    "Let's first plot this function to gain some intuition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695a3b78-548a-4576-8385-8ea8d42d50c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Test_2D(X):\n",
    "    \"\"\"2D Test Function\"\"\"\n",
    "    y = 20 - (X[:,0]-X[:,1])**2 - 8*(X[:,0]+X[:,1]-4)**3\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75c630b-e988-40d7-a413-1b65ec161f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LHS package\n",
    "from pyDOE import lhs\n",
    "\n",
    "# Test data\n",
    "X1 = np.linspace(-5, 5, 100)\n",
    "X2 = np.linspace(-5, 5, 100)\n",
    "X1, X2 = np.meshgrid(X1, X2)\n",
    "X_test = np.hstack((X1.reshape(-1,1), X2.reshape(-1,1)))\n",
    "y_test = Test_2D(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e932b8e3-c402-4b2e-b8eb-3b1c9417c4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the limit state function\n",
    "fig, ax = plt.subplots(figsize=(7,5))\n",
    "ax.contour(X_test[:,0].reshape(100,-1), X_test[:,1].reshape(100,-1), \n",
    "           y_test.reshape(100,-1), levels = [0], \n",
    "           colors='r',linestyles='--',linewidths=2)\n",
    "ax.set_xlabel(r'$X_1$', fontsize=15)\n",
    "ax.set_ylabel(r'$X_2$', fontsize=15)\n",
    "ax.set_title('Limit-State Function', fontsize=15)\n",
    "ax.tick_params(axis='both', which='major', labelsize=12);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f77024-4f99-491e-b6b7-2dffae786ff8",
   "metadata": {},
   "source": [
    "### 2. Generate Training Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73addf59-7f04-443f-8ffd-15289e982905",
   "metadata": {},
   "source": [
    "#### 2.1 Train an initial model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39bc3a3-bd47-49e5-89b1-3301953791ac",
   "metadata": {},
   "source": [
    "As usual, we use **Latin Hypercube Sampling** approach to generate training samples. We use 15 samples to train an initial GP model. Remember that it is a good practice to normalize the inputs before submitting to GP training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8eea28-e498-4914-a254-c538994d7147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training data\n",
    "sample_num = 15\n",
    "lb, ub = np.array([-5, -5]), np.array([5, 5])\n",
    "X_train = (ub-lb)*qmc.LatinHypercube(d=2, seed=42).random(sample_num) + lb\n",
    "\n",
    "# Compute labels\n",
    "y_train = Test_2D(X_train).reshape(-1,1)\n",
    "\n",
    "# Scale input\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97613bb2-c9bf-453f-a2c1-b7af76295ac3",
   "metadata": {},
   "source": [
    "Next, let's create a couple of helper functions to streamline the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86070df-2165-424c-9b22-cb851a3cca1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def config_GP(lengthscale, X_train, y_train):\n",
    "    \"\"\"\n",
    "    Initialize the kernel parameters.\n",
    "\n",
    "    Args:\n",
    "    -----\n",
    "    lengthscale: initial length scale values for GP model.\n",
    "    X_train: training input data.\n",
    "    y_train: training output data.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    model: configured GP model using GPflow.\n",
    "    \"\"\"  \n",
    "    \n",
    "    # Set up the kernel\n",
    "    kernel = gpflow.kernels.SquaredExponential(variance=np.var(y_train), lengthscales=lengthscale)\n",
    "\n",
    "    # Set up the model\n",
    "    model = gpflow.models.GPR(\n",
    "        (X_train, y_train.reshape(-1, 1)),\n",
    "        kernel=kernel,\n",
    "        mean_function=gpflow.functions.Polynomial(0),\n",
    "        noise_variance=1e-5\n",
    "    )\n",
    "    \n",
    "    gpflow.set_trainable(model.likelihood.variance, False)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a56ee4e-79cd-42b5-b898-c9a01a2c5b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_kernel_params(dim, n_restarts=5):\n",
    "    \"\"\"\n",
    "    Initialize the kernel parameters.\n",
    "\n",
    "    Args:\n",
    "    -----\n",
    "    dim: input dimension. One length scale for each dimension.\n",
    "    n_restarts: number of random restarts for length scale optimization.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    length_scales: random guesses of the length scale parameters.\n",
    "    \"\"\"  \n",
    "    \n",
    "    lb, ub = -2, 2\n",
    "    lhd = qmc.LatinHypercube(d=dim, seed=42).random(n_restarts)\n",
    "    length_scales = (ub-lb)*lhd + lb\n",
    "    length_scales = 10**length_scales\n",
    "\n",
    "    return length_scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194a09ac-ce66-4914-8f90-955e518e7881",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(X_train, y_train, n_restarts=5):\n",
    "    \n",
    "    models = []\n",
    "    log_likelihoods = []\n",
    "    init_length_scales = init_kernel_params(dim=X_train.shape[1], n_restarts=n_restarts)\n",
    "    \n",
    "    with tf.device(\"CPU:0\"):\n",
    "    \n",
    "        for i, length_scale in enumerate(init_length_scales):\n",
    "    \n",
    "            # Init model\n",
    "            model = config_GP(length_scale, X_train, y_train)\n",
    "    \n",
    "            # Training\n",
    "            opt = gpflow.optimizers.Scipy()\n",
    "            opt.minimize(model.training_loss, model.trainable_variables, options=dict(maxiter=100))\n",
    "    \n",
    "            # Record keeping\n",
    "            models.append(model)\n",
    "            log_likelihoods.append(model.log_marginal_likelihood().numpy())\n",
    "    \n",
    "    # Select the model with the highest log-marginal likelihood\n",
    "    best_model_index = np.argmax(log_likelihoods)\n",
    "    best_model = models[best_model_index]\n",
    "    \n",
    "    print(f\"Best model log-marginal likelihood: {log_likelihoods[best_model_index]}\")\n",
    "\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70369b3-52e5-4f8c-9ccf-608962e63ec2",
   "metadata": {},
   "source": [
    "Let's fit an initial GP model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cc037b-b588-4f00-9833-5f338f3ce891",
   "metadata": {},
   "outputs": [],
   "source": [
    "GP = fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f651c071-765c-4dbb-a88a-625d922d6caf",
   "metadata": {},
   "source": [
    "Then, we assess the performance of the fitted model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda5b9dc-65dc-4a5a-ad8c-68c8175e5369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GP model predicting\n",
    "f_mean, _ = GP.predict_f(scaler.transform(X_test))\n",
    "f_mean = f_mean.numpy().flatten()\n",
    "\n",
    "# Post-processing - Contour plot\n",
    "fig, ax = plt.subplots(figsize=(7,5))\n",
    "\n",
    "# Reference results\n",
    "ax.contour(X_test[:,0].reshape(100,-1), \n",
    "           X_test[:,1].reshape(100,-1), \n",
    "           y_test.reshape(100,-1), levels = [0],\n",
    "           colors='r',linestyles='--',linewidths=2)\n",
    "\n",
    "# GP predictions\n",
    "ax.contour(X_test[:,0].reshape(100,-1), \n",
    "           X_test[:,1].reshape(100,-1), \n",
    "           f_mean.reshape(100, -1), levels = [0],\n",
    "           colors='k',linestyles='-',linewidths=2)\n",
    "\n",
    "ax.plot(X_train[:,0], X_train[:,1],'bo', \n",
    "        markerfacecolor='b', markersize=10)\n",
    "\n",
    "ax.set_xlabel(r'$X_1$', fontsize=15)\n",
    "ax.set_ylabel(r'$X_2$', fontsize=15)\n",
    "ax.set_title('Limit-State Function', fontsize=15)\n",
    "ax.tick_params(axis='both', which='major', labelsize=12);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495db01e-1fc9-4993-9a05-42638d2705dd",
   "metadata": {},
   "source": [
    "Obviously, the obtained limit state (or called stability margin) is far from correct. In the following, we use **U-learning** to gradually refine the GP approximation accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9873e9-0e67-4f86-8e6a-42e340593351",
   "metadata": {},
   "source": [
    "#### 2.2 Acquisition function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4330d3-e9d8-429f-90be-84fa6f830e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def acquisition(model, candidate, limit_state_value=0, diagnose=False):\n",
    "\n",
    "    # Compute prediction variance\n",
    "    f_mean, f_var = model.predict_f(candidate, full_cov=False)\n",
    "    f_mean = f_mean.numpy().flatten()\n",
    "    f_var = f_var.numpy().flatten()\n",
    "\n",
    "    # Calculate U values\n",
    "    U_values = np.abs(f_mean-limit_state_value)/np.sqrt(f_var)\n",
    "    target = np.min(U_values)\n",
    "\n",
    "    # Locate promising sample\n",
    "    index = np.argmin(U_values)\n",
    "\n",
    "    # Select promising sample\n",
    "    sample = candidate[[index],:]\n",
    "    reduced_candidate = np.delete(candidate, obj=index, axis=0)\n",
    "\n",
    "    # For diagnose purposes\n",
    "    diagnostics = U_values\n",
    "\n",
    "    if diagnose is True:\n",
    "        return target, sample, candidate, reduced_candidate, diagnostics\n",
    "    else:\n",
    "        return target, sample, candidate, reduced_candidate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c8b82e-aa26-4b38-8c3c-dd21a91d933c",
   "metadata": {},
   "source": [
    "#### 2.3 Iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9536d828-bbac-4d17-b75d-7e8996f526f2",
   "metadata": {},
   "source": [
    "Before we start iterating, we need to generate a pool of candidate samples. We will maintain this candidate pool for the subsequent iterations. At each iteration, we pick one sample from the candidate pool. This sample should yield the maximum expected prediction error value among all the candidate samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9f6daf-35b6-4bbb-8ced-eca4e5054581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start active learning\n",
    "iteration = 1\n",
    "U_history = []\n",
    "\n",
    "# Generate candidate samples (in normalized scale)\n",
    "Pool = np.random.rand(10000, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0963aeeb-88b7-4d7f-b9a2-5fdce29c00e4",
   "metadata": {},
   "source": [
    "Now we are ready for the first iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf098b9-507e-42ad-8635-8057b87c7340",
   "metadata": {},
   "source": [
    "**The following three cells can be excuted multiple times to manually control the iteration flow**. The first cell identify the sample with the minimum U value. The second and third cell visually summarizes the results from the current iteration and add newly identified samples to the current training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2166d20c-e201-40a5-a3b4-d05b34e499e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-GP model training and predicting\n",
    "GP = fit(X_train_scaled, y_train, n_restarts=5)\n",
    "f_mean, f_var = GP.predict_f(scaler.transform(X_test), full_cov=False)\n",
    "f_upper = f_mean + 3*np.sqrt(f_var)\n",
    "f_lower = f_mean - 3*np.sqrt(f_var)\n",
    "\n",
    "# 2-Active learning\n",
    "target, sample, org_pool, Pool, U = acquisition(model=GP, candidate=Pool, \n",
    "                                                limit_state_value=0, diagnose=True)\n",
    "\n",
    "# Record keeping\n",
    "U_history.append(target)\n",
    "\n",
    "# 3-Display iteration info\n",
    "summary = 'Iteration summary:'\n",
    "iter_number = 'Current iteration: {}'.format(str(iteration))\n",
    "\n",
    "Iteration_summary = 'Iteration {}:'.format(str(iteration)) \\\n",
    "                    + os.linesep \\\n",
    "                    + 'Current min U is {}'.format(str(target)) \n",
    "\n",
    "print(Iteration_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23144d2a-d3f2-4692-b52b-4035ad92c700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4-Iteration assessment\n",
    "fig, ax = plt.subplots(figsize=(7,5))\n",
    "\n",
    "display_y = [y_test, f_mean, f_upper, f_lower]\n",
    "colors = ['r', 'k', 'k', 'k']\n",
    "linestyles = ['-', '-', '--', '--']\n",
    "\n",
    "for i in range(4):\n",
    "    ax.contour(X_test[:,0].reshape(100,-1), X_test[:,1].reshape(100,-1),\n",
    "               display_y[i].reshape(100,-1), levels = [0],\n",
    "               colors=colors[i], linestyles=linestyles[i], linewidths=2)\n",
    "\n",
    "ax.plot(X_train[:sample_num, 0], X_train[:sample_num, 1],'bo', \n",
    "        markerfacecolor='b', markersize=10)\n",
    "\n",
    "# 5-Enrich training dataset\n",
    "org_sample = scaler.inverse_transform(sample)  # Sample in original scale\n",
    "X_train = np.vstack((X_train, org_sample))\n",
    "y_train = np.vstack((y_train, Test_2D(org_sample)))\n",
    "iteration += 1\n",
    "\n",
    "# Newly enriched samples\n",
    "ax.plot(scaler.inverse_transform(sample)[sample_num:, 0], X_train[sample_num:, 1],'ro', \n",
    "        markerfacecolor='r', markersize=10)\n",
    "\n",
    "ax.set_xlabel(r'$X_1$', fontsize=15)\n",
    "ax.set_ylabel(r'$X_2$', fontsize=15)\n",
    "ax.set_title('Limit-State Function', fontsize=15)\n",
    "ax.tick_params(axis='both', which='major', labelsize=12);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beaba013-38b7-4014-90c1-6a487f2941c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# U evolution history\n",
    "fig, ax = plt.subplots(figsize=(7,5))\n",
    "\n",
    "threshold = 1.65\n",
    "ax.plot(np.arange(1, iteration), U_history, 'k-o', lw=2)\n",
    "ax.plot([0, 32.3], [threshold, threshold], 'r--', lw=2)\n",
    "ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "ax.set_xlabel('Iteration', fontsize=15)\n",
    "ax.set_ylabel('Min U', fontsize=15)\n",
    "ax.set_title('Evolution', fontsize=15)\n",
    "ax.set_xlim([0.8, 32.2]);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
